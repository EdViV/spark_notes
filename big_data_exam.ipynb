{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4a2753-9da5-4203-9b7c-432e95afd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/evivancovid/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'evivancovid | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911e1cc-32a8-47f7-960f-3dac2027ad1c",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f679d770-c2e2-4969-988d-d11d4388895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/public/data/nyse_years/*\", \n",
    "                   schema = \"stockticker STRING, tradedate STRING, openprice FLOAT, highprice FLOAT, lowprice FLOAT, closeprice FLOAT, volume INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05a64cf-4973-4eea-ad58-534b6654cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list =[(df.count(),)]\n",
    "\n",
    "traded_stocks = spark.createDataFrame(count_list).toDF(\"traded_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824b2335-a791-4825-9ce6-3592c01bba16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(traded_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33aadb6d-30bf-485d-9732-4071fb2cf1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://localhost:9000/user/evivancovid/spark_practice/problem1/data/nyse_count already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-18b8d6797a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraded_stocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/evivancovid/spark_practice/problem1/data/nyse_count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://localhost:9000/user/evivancovid/spark_practice/problem1/data/nyse_count already exists."
     ]
    }
   ],
   "source": [
    "traded_stocks.coalesce(1).write.csv(\"/user/evivancovid/spark_practice/problem1/data/nyse_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e734ee7-2ef2-4802-849e-0e1713089eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 evivancovid supergroup          0 2022-05-20 00:15 /user/evivancovid/spark_practice/problem1/data/nyse_count/_SUCCESS\n",
      "-rw-r--r--   1 evivancovid supergroup          8 2022-05-20 00:15 /user/evivancovid/spark_practice/problem1/data/nyse_count/part-00000-3e7efb14-11ea-443d-847c-ddb7877fb5db-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/evivancovid/spark_practice/problem1/data/nyse_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bb5a1-0725-401d-a6a1-78b37721143e",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3810b68b-1afd-4cf1-8baa-502f84e33f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2DF = spark.read.csv(\"/public/data/nyse_years/NYSE_2010.txt\", schema = \"stockticker STRING, tradedate STRING, openprice FLOAT, highprice FLOAT, lowprice FLOAT, closeprice FLOAT, volume INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b34152c5-f896-4ebf-92cc-0ffb287341a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2DF_stockticker = p2DF.select(\"stockticker\").distinct().orderBy(\"stockticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e4d25f7-8dbd-47a1-a1a2-9d42b8449995",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2DF_stockticker.coalesce(1).write.csv(\"/user/evivancovid/spark_practice/problem2/data/unique_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09b587e0-3354-449d-b3bf-d0f49801f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 evivancovid supergroup          0 2022-05-20 00:29 /user/evivancovid/spark_practice/problem2/data/unique_stocks/_SUCCESS\n",
      "-rw-r--r--   1 evivancovid supergroup       8055 2022-05-20 00:29 /user/evivancovid/spark_practice/problem2/data/unique_stocks/part-00000-4785bccf-39b1-4895-bcf0-7a29fe11b435-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/evivancovid/spark_practice/problem2/data/unique_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72997e2b-0ec8-40fe-ab95-d4219f47a59a",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe512d2-eb37-4d7f-915e-5d5551517034",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/public/data/nyse_years/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37deddd3-b6d1-4287-ba25-5a3e0f271d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_data_all = spark.read.csv(path, schema = \"stockticker STRING, tradedate INT, openprice FLOAT, highprice FLOAT, lowprice FLOAT, closeprice FLOAT, volume LONG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614b0b40-db4f-4e48-a680-bfbc5dee569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+---------+--------+----------+------+\n",
      "|stockticker|tradedate|openprice|highprice|lowprice|closeprice|volume|\n",
      "+-----------+---------+---------+---------+--------+----------+------+\n",
      "|          A| 20160101|    41.81|    41.81|   41.81|     41.81|     0|\n",
      "|         AA| 20160101|    29.61|    29.61|   29.61|     29.61|     0|\n",
      "|        AAC| 20160101|    19.06|    19.06|   19.06|     19.06|     0|\n",
      "|        AAN| 20160101|    22.39|    22.39|   22.39|     22.39|     0|\n",
      "|        AAP| 20160101|   150.51|   150.51|  150.51|    150.51|     0|\n",
      "|        AAT| 20160101|    38.35|    38.35|   38.35|     38.35|     0|\n",
      "|        AAV| 20160101|     5.08|     5.08|    5.08|      5.08|     0|\n",
      "|         AB| 20160101|    23.85|    23.85|   23.85|     23.85|     0|\n",
      "|        ABB| 20160101|    17.73|    17.73|   17.73|     17.73|     0|\n",
      "|       ABBV| 20160101|    59.24|    59.24|   59.24|     59.24|     0|\n",
      "|        ABC| 20160101|   103.71|   103.71|  103.71|    103.71|     0|\n",
      "|       ABEV| 20160101|     4.46|     4.46|    4.46|      4.46|     0|\n",
      "|        ABG| 20160101|    67.44|    67.44|   67.44|     67.44|     0|\n",
      "|        ABM| 20160101|    28.47|    28.47|   28.47|     28.47|     0|\n",
      "|        ABR| 20160101|     7.15|     7.15|    7.15|      7.15|     0|\n",
      "|      ABR-A| 20160101|     24.5|     24.5|    24.5|      24.5|     0|\n",
      "|      ABR-B| 20160101|    22.56|    22.56|   22.56|     22.56|     0|\n",
      "|      ABR-C| 20160101|    25.17|    25.17|   25.17|     25.17|     0|\n",
      "|       ABRN| 20160101|     24.6|     24.6|    24.6|      24.6|     0|\n",
      "|        ABT| 20160101|    44.91|    44.91|   44.91|     44.91|     0|\n",
      "+-----------+---------+---------+---------+--------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyse_data_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "140c948f-61db-47a5-86fa-f6109a5806a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_data_close = nyse_data_all.select(\"stockticker\",\"closeprice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e742ec0-49cf-4d6e-b5ad-4214a5026ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_data_close.repartition(8).write.json(\"/user/evivancovid/spark_practice/problem3/data/nyse_data_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc37894a-99da-4d10-a811-3cce23cf4018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "-rw-r--r--   1 evivancovid supergroup          0 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/_SUCCESS\n",
      "-rw-r--r--   1 evivancovid supergroup   47976393 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00000-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n",
      "-rw-r--r--   1 evivancovid supergroup   47974676 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00001-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n",
      "-rw-r--r--   1 evivancovid supergroup   47975921 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00002-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n",
      "-rw-r--r--   1 evivancovid supergroup   47976439 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00003-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n",
      "-rw-r--r--   1 evivancovid supergroup   47975986 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00004-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n",
      "-rw-r--r--   1 evivancovid supergroup   47977563 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00005-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n",
      "-rw-r--r--   1 evivancovid supergroup   47977511 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00006-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n",
      "-rw-r--r--   1 evivancovid supergroup   47977547 2022-05-20 00:51 /user/evivancovid/spark_practice/problem3/data/nyse_data_json/part-00007-11ab0989-8cd9-4bf4-bf92-1b82ece77196-c000.json\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/evivancovid/spark_practice/problem3/data/nyse_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b723c-7baa-4ae1-8237-3a17a0295076",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48f17e51-64d9-4bc3-a5d9-62b990b2937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/public/data/nyse_years/*\"\n",
    "\n",
    "path2 = \"/public/data/nyse_stocks/*\"\n",
    "\n",
    "nyse_data_all = spark.read.csv(path, schema = \"stockticker STRING, tradedate INT, openprice FLOAT, highprice FLOAT, lowprice FLOAT, closeprice FLOAT, volume LONG\")\n",
    "nyse_meta = spark.read.csv(path2, sep = \"|\", header = False, schema = \"stockticker_meta STRING, stockname STRING, openprice FLOAT, volume_meta FLOAT, smthg1 FLOAT, smthg2  FLOAT, industry STRING, business STRING, webpage STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53862e67-ab7c-4063-9078-a9dfbd41fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_joined = nyse_data_all.join(nyse_meta, on = nyse_data_all.stockticker == nyse_meta.stockticker_meta, how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc8c939f-0bf9-408e-9fdc-de21c17c8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "never_traded_stocks = nyse_joined.select(\"stockname, volume\").distinct().where(\"volume == 0\").orderBy(\"stockname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7c3a6b9-25ef-4a30-acd1-2ac40b1ddc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>stockname</th></tr>\n",
       "<tr><td>null</td></tr>\n",
       "<tr><td>3D Systems Corpor...</td></tr>\n",
       "<tr><td>3M Company</td></tr>\n",
       "<tr><td>A.H. Belo Corpora...</td></tr>\n",
       "<tr><td>A10 Networks, Inc.</td></tr>\n",
       "<tr><td>AAR Corp.</td></tr>\n",
       "<tr><td>ABB Ltd</td></tr>\n",
       "<tr><td>ABM Industries In...</td></tr>\n",
       "<tr><td>AGCO Corporation</td></tr>\n",
       "<tr><td>AK Steel Holding ...</td></tr>\n",
       "<tr><td>AMN Healthcare Se...</td></tr>\n",
       "<tr><td>AMREP Corporation</td></tr>\n",
       "<tr><td>AMTEK, Inc.</td></tr>\n",
       "<tr><td>ARC Document Solu...</td></tr>\n",
       "<tr><td>ARMOUR Residentia...</td></tr>\n",
       "<tr><td>ASA Gold and Prec...</td></tr>\n",
       "<tr><td>AT&amp;T Inc.</td></tr>\n",
       "<tr><td>AU Optronics Corp</td></tr>\n",
       "<tr><td>AVG Technologies ...</td></tr>\n",
       "<tr><td>AVX Corporation</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|           stockname|\n",
       "+--------------------+\n",
       "|                null|\n",
       "|3D Systems Corpor...|\n",
       "|          3M Company|\n",
       "|A.H. Belo Corpora...|\n",
       "|  A10 Networks, Inc.|\n",
       "|           AAR Corp.|\n",
       "|             ABB Ltd|\n",
       "|ABM Industries In...|\n",
       "|    AGCO Corporation|\n",
       "|AK Steel Holding ...|\n",
       "|AMC Entertainment...|\n",
       "|AMN Healthcare Se...|\n",
       "|   AMREP Corporation|\n",
       "|         AMTEK, Inc.|\n",
       "|ARC Document Solu...|\n",
       "|ASA Gold and Prec...|\n",
       "|           AT&T Inc.|\n",
       "|   AU Optronics Corp|\n",
       "|AVG Technologies ...|\n",
       "|     AVX Corporation|\n",
       "+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "never_traded_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6eca678-d840-410e-a35f-809142eb31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "never_traded_stocks.repartition(1).write.text(\"/user/evivancovid/spark_practice/problem5/data/untraded_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0bbc273-8764-4ada-a90c-738d19eea51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 evivancovid supergroup          0 2022-05-24 14:41 /user/evivancovid/spark_practice/problem5/data/untraded_stocks/_SUCCESS\n",
      "-rw-r--r--   1 evivancovid supergroup      54535 2022-05-24 14:41 /user/evivancovid/spark_practice/problem5/data/untraded_stocks/part-00000-68616906-131d-4d41-899e-e45dff38f6ad-c000.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/evivancovid/spark_practice/problem5/data/untraded_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f1bf1-f65f-4235-8196-763dd82abaca",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5944b9d5-74c2-445b-88f0-f8566218ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/public/data/nyse_years/*\"\n",
    "\n",
    "path2 = \"/public/data/nyse_stocks/*\"\n",
    "\n",
    "nyse_data_all = spark.read.csv(path, schema = \"stockticker STRING, tradedate INT, openprice FLOAT, highprice FLOAT, lowprice FLOAT, closeprice FLOAT, volume LONG\")\n",
    "nyse_meta = spark.read.csv(path2, sep = \"|\", header = False, schema = \"stockticker_meta STRING, stockname STRING, openprice FLOAT, volume FLOAT, smthg1 FLOAT, smthg2  FLOAT, industry STRING, business STRING, webpage STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f67759-744b-42a0-887e-01d655cee627",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_data_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c260e6-abe9-4366-a02d-bbbd39c1ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_joined = nyse_data_all.join(nyse_meta, on = nyse_data_all.stockticker == nyse_meta.stockticker_meta, how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1108ea14-6c6b-4c1c-9447-a35cbca1322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>stockticker</th><th>tradedate</th><th>openprice</th><th>highprice</th><th>lowprice</th><th>closeprice</th><th>volume</th><th>stockticker_meta</th><th>stockname</th><th>openprice</th><th>volume</th><th>smthg1</th><th>smthg2</th><th>industry</th><th>business</th><th>webpage</th></tr>\n",
       "<tr><td>BOX</td><td>20130101</td><td>18.85</td><td>18.85</td><td>18.85</td><td>18.85</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130102</td><td>19.22</td><td>19.37</td><td>19.1</td><td>19.24</td><td>87700</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130103</td><td>19.27</td><td>19.27</td><td>19.0</td><td>19.12</td><td>36100</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130104</td><td>19.12</td><td>19.35</td><td>19.07</td><td>19.16</td><td>47600</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130107</td><td>19.14</td><td>19.28</td><td>19.0</td><td>19.04</td><td>63600</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130108</td><td>19.15</td><td>19.23</td><td>19.0</td><td>19.17</td><td>53600</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130109</td><td>19.28</td><td>19.57</td><td>19.1</td><td>19.42</td><td>76100</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130110</td><td>19.58</td><td>19.61</td><td>19.31</td><td>19.48</td><td>73100</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130111</td><td>19.52</td><td>19.64</td><td>19.43</td><td>19.55</td><td>71500</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130114</td><td>19.55</td><td>19.62</td><td>19.36</td><td>19.48</td><td>44800</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130115</td><td>19.42</td><td>19.6</td><td>19.31</td><td>19.54</td><td>73400</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130116</td><td>19.56</td><td>19.64</td><td>19.5</td><td>19.5</td><td>61200</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130117</td><td>19.52</td><td>19.79</td><td>19.52</td><td>19.72</td><td>134100</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130118</td><td>19.85</td><td>20.55</td><td>19.74</td><td>20.3</td><td>243600</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130121</td><td>20.3</td><td>20.3</td><td>20.3</td><td>20.3</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130122</td><td>22.96</td><td>23.15</td><td>22.96</td><td>23.14</td><td>3299800</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130123</td><td>22.9</td><td>22.99</td><td>22.85</td><td>22.94</td><td>1135400</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130124</td><td>22.92</td><td>22.92</td><td>22.82</td><td>22.87</td><td>642900</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130125</td><td>22.88</td><td>22.95</td><td>22.86</td><td>22.87</td><td>248300</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>BOX</td><td>20130128</td><td>22.87</td><td>22.88</td><td>22.87</td><td>22.88</td><td>186800</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+---------+---------+---------+--------+----------+-------+----------------+---------+---------+------+------+------+--------+--------+-------+\n",
       "|stockticker|tradedate|openprice|highprice|lowprice|closeprice| volume|stockticker_meta|stockname|openprice|volume|smthg1|smthg2|industry|business|webpage|\n",
       "+-----------+---------+---------+---------+--------+----------+-------+----------------+---------+---------+------+------+------+--------+--------+-------+\n",
       "|        BOX| 20130101|    18.85|    18.85|   18.85|     18.85|      0|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130102|    19.22|    19.37|    19.1|     19.24|  87700|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130103|    19.27|    19.27|    19.0|     19.12|  36100|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130104|    19.12|    19.35|   19.07|     19.16|  47600|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130107|    19.14|    19.28|    19.0|     19.04|  63600|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130108|    19.15|    19.23|    19.0|     19.17|  53600|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130109|    19.28|    19.57|    19.1|     19.42|  76100|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130110|    19.58|    19.61|   19.31|     19.48|  73100|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130111|    19.52|    19.64|   19.43|     19.55|  71500|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130114|    19.55|    19.62|   19.36|     19.48|  44800|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130115|    19.42|     19.6|   19.31|     19.54|  73400|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130116|    19.56|    19.64|    19.5|      19.5|  61200|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130117|    19.52|    19.79|   19.52|     19.72| 134100|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130118|    19.85|    20.55|   19.74|      20.3| 243600|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130121|     20.3|     20.3|    20.3|      20.3|      0|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130122|    22.96|    23.15|   22.96|     23.14|3299800|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130123|     22.9|    22.99|   22.85|     22.94|1135400|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130124|    22.92|    22.92|   22.82|     22.87| 642900|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130125|    22.88|    22.95|   22.86|     22.87| 248300|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "|        BOX| 20130128|    22.87|    22.88|   22.87|     22.88| 186800|            null|     null|     null|  null|  null|  null|    null|    null|   null|\n",
       "+-----------+---------+---------+---------+--------+----------+-------+----------------+---------+---------+------+------+------+--------+--------+-------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyse_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2725ad3b-8440-49df-a1e0-2b9f305dfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stock_name = nyse_joined.select(\"stockticker\").distinct().where(\"stockname is null\").orderBy(\"stockticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7679ae1-e5b2-4485-9be0-086a63410af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>stockticker</th></tr>\n",
       "<tr><td>AA-B</td></tr>\n",
       "<tr><td>AAC</td></tr>\n",
       "<tr><td>ABR-A</td></tr>\n",
       "<tr><td>ABR-B</td></tr>\n",
       "<tr><td>ABR-C</td></tr>\n",
       "<tr><td>AC</td></tr>\n",
       "<tr><td>ACV</td></tr>\n",
       "<tr><td>ADNT</td></tr>\n",
       "<tr><td>ADPT</td></tr>\n",
       "<tr><td>ADSW</td></tr>\n",
       "<tr><td>AES-C</td></tr>\n",
       "<tr><td>AEUA</td></tr>\n",
       "<tr><td>AF-C</td></tr>\n",
       "<tr><td>AFGE</td></tr>\n",
       "<tr><td>AFGH</td></tr>\n",
       "<tr><td>AFI</td></tr>\n",
       "<tr><td>AFS-A</td></tr>\n",
       "<tr><td>AFS-B</td></tr>\n",
       "<tr><td>AFS-C</td></tr>\n",
       "<tr><td>AFS-D</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+\n",
       "|stockticker|\n",
       "+-----------+\n",
       "|       AA-B|\n",
       "|        AAC|\n",
       "|      ABR-A|\n",
       "|      ABR-B|\n",
       "|      ABR-C|\n",
       "|         AC|\n",
       "|        ACV|\n",
       "|       ADNT|\n",
       "|       ADPT|\n",
       "|       ADSW|\n",
       "|      AES-C|\n",
       "|       AEUA|\n",
       "|       AF-C|\n",
       "|       AFGE|\n",
       "|       AFGH|\n",
       "|        AFI|\n",
       "|      AFS-A|\n",
       "|      AFS-B|\n",
       "|      AFS-C|\n",
       "|      AFS-D|\n",
       "+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stock_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f37e16-50d0-4d88-8362-69d67d0fc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stock_name.repartition(1).write.text(\"/user/evivancovid/spark_practice/problem4/data/no_stock_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8647c45-236e-49dd-8538-6d9e09ff0824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 evivancovid supergroup          0 2022-05-20 01:31 /user/evivancovid/spark_practice/problem4/data/no_stock_names/_SUCCESS\n",
      "-rw-r--r--   1 evivancovid supergroup       4814 2022-05-20 01:31 /user/evivancovid/spark_practice/problem4/data/no_stock_names/part-00000-b31137c7-048f-47ea-9047-ed14f98f8c9b-c000.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/evivancovid/spark_practice/problem4/data/no_stock_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f508c013-a428-486f-b35b-51ba4b5ee6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method distinct in module pyspark.sql.dataframe:\n",
      "\n",
      "distinct() method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.distinct().count()\n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nyse_data_all.distinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55883b-1eb2-4574-8c10-0abbed427bec",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6add090-0a27-4e8f-9308-e43cdc8808a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/public/data/nyse_years/*\"\n",
    "\n",
    "path2 = \"/public/data/nyse_stocks/*\"\n",
    "\n",
    "nyse_data_all = spark.read.csv(path, schema = \"stockticker STRING, tradedate STRING, openprice STRING, highprice STRING, lowprice STRING, closeprice STRING, volume STRING\")\n",
    "nyse_meta = spark.read.csv(path2, sep = \"|\", header = False, schema = \"stockticker_meta STRING, stockname STRING, open_price FLOAT, volume_meta FLOAT, smthg1 FLOAT, smthg2  FLOAT, industry STRING, business STRING, webpage STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9771c9a-e864-42a7-bef3-598639aaa56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_joined = nyse_data_all.join(nyse_meta, on = nyse_data_all.stockticker == nyse_meta.stockticker_meta, how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "480e7037-5ef0-403c-8676-360ed0e611d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, concat, lit\n",
    "\n",
    "name_na_df = nyse_joined.select(\"*\"). \\\n",
    "    withColumn('stockname', expr(\"nvl(nullif(stockname, ''), 'Stock Name Not Available')\")). \\\n",
    "    drop(\"stockticker_meta\", \"open_price\", \"volume_meta\", \"smthg1\", \"smthg2\", \"industry\", \"business\", \"webpage\"). \\\n",
    "    orderBy(\"tradedate\", \"stockticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11521b47-6261-4c03-8a3d-aacda40163aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o361.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:375)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3119/21514368.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset$$Lambda$3075/1391935165.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.Dataset$$Lambda$1641/1433871220.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1649/1497936359.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1642/2093425166.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-97facb8520f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mname_na_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stockticker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tradedate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"openprice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"highprice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lowprice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"closeprice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"volume\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"stockname\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name_na\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o361.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:375)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3119/21514368.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset$$Lambda$3075/1391935165.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.Dataset$$Lambda$1641/1433871220.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1649/1497936359.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1642/2093425166.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n"
     ]
    }
   ],
   "source": [
    "name_na_df.select(concat(\"stockticker\", lit(\";\"),\"tradedate\", lit(\";\"),\"openprice\", lit(\";\"),\"highprice\", lit(\";\"),\"lowprice\", lit(\";\"),\"closeprice\", lit(\";\"),\"volume\", lit(\";\"),\"stockname\")).collect().toDF(\"name_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bbb3f5c-f87c-4775-bdbc-b87780f9dbde",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'toDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a99c7a9581e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_na_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'toDF'"
     ]
    }
   ],
   "source": [
    "name_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ed0b43c-3b1e-4f5d-b4d0-15a2488fea81",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Text data source supports only a single column, and you have 8 columns.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-c1b9c0444385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mname_not_available\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/evivancovid/spark_practice/problem6/data/stock_data_with_names\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, path, compression, lineSep)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \"\"\"\n\u001b[1;32m   1272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m     def csv(self, path, mode=None, compression=None, sep=None, quote=None, escape=None,\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Text data source supports only a single column, and you have 8 columns."
     ]
    }
   ],
   "source": [
    "name_not_available.repartition(8).write.text(\"/user/evivancovid/spark_practice/problem6/data/stock_data_with_names\", lineSep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "458fc7a9-587b-461f-a5f6-8b76a3e36fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sort in module pyspark.sql.dataframe:\n",
      "\n",
      "sort(*cols, **kwargs) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols : str, list, or :class:`Column`, optional\n",
      "         list of :class:`Column` or column names to sort by.\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    ascending : bool or list, optional\n",
      "        boolean or list of boolean (default ``True``).\n",
      "        Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "        If a list is specified, length of the list must equal length of the `cols`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.sort(df.age.desc()).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.sort(\"age\", ascending=False).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.orderBy(df.age.desc()).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> from pyspark.sql.functions import *\n",
      "    >>> df.sort(asc(\"age\")).collect()\n",
      "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "    >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.orderBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e6524-9e66-4b55-9d05-b01270d59f32",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95942a74-448d-499d-8d1c-6717f222be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/public/data/nyse_years/*\"\n",
    "\n",
    "nyse_data_all = spark.read.csv(path, schema = \"stockticker STRING, tradedate INT, openprice FLOAT, highprice FLOAT, lowprice FLOAT, closeprice FLOAT, volume LONG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b9dc51-095e-4182-87ff-80452d578c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"USE evivancovid_nyvenv\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2353ff6d-a672-4b58-b084-4b253f59edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "sorted_data_nyse = nyse_data_all.orderBy(col(\"tradedate\"), col(\"volume\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cab43e4-c2c4-439c-9664-af786251ba75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5a7549defd20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_data_nyse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nyse_data_avro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\"."
     ]
    }
   ],
   "source": [
    "sorted_data_nyse.repartition(4).write.format(\"avro\").saveAsTable(\"nyse_data_avro\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fac6e68-2eeb-4f30-b4e3-9eb83956df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_avro in module pyspark.sql.avro.functions:\n",
      "\n",
      "to_avro(data, jsonFormatSchema='')\n",
      "    Converts a column into binary of avro format.\n",
      "    \n",
      "    .. versionadded:: 3.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`~pyspark.sql.Column` or str\n",
      "        the data column.\n",
      "    jsonFormatSchema : str, optional\n",
      "        user-specified output avro schema in JSON string format.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Avro is built-in but external data source module since Spark 2.4. Please deploy the\n",
      "    application as per the deployment section of \"Apache Avro Data Source Guide\".\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> from pyspark.sql.avro.functions import to_avro\n",
      "    >>> data = ['SPADES']\n",
      "    >>> df = spark.createDataFrame(data, \"string\")\n",
      "    >>> df.select(to_avro(df.value).alias(\"suite\")).collect()\n",
      "    [Row(suite=bytearray(b'\\x00\\x0cSPADES'))]\n",
      "    \n",
      "    >>> jsonFormatSchema = '''[\"null\", {\"type\": \"enum\", \"name\": \"value\",\n",
      "    ...     \"symbols\": [\"SPADES\", \"HEARTS\", \"DIAMONDS\", \"CLUBS\"]}]'''\n",
      "    >>> df.select(to_avro(df.value, jsonFormatSchema).alias(\"suite\")).collect()\n",
      "    [Row(suite=bytearray(b'\\x02\\x00'))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_avro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35217992-9716-4f05-ad0c-7f80f451813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `cast` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b3b37-700f-47d2-a3b7-74765bcf9e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
