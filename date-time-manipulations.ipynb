{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6922a3-21c8-46a5-b033-0df765e2b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/evivancovid/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'evivancovid | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89f23fb-71b6-41db-a9e8-3445272e512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, lit, to_date, to_timestamp\n",
    "\n",
    "l = [(\"X\", )]\n",
    "df = spark.createDataFrame(l).toDF(\"dummy\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927ee85-ef91-479d-811f-0242aab4d370",
   "metadata": {},
   "source": [
    "# Date and Time common manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5d8620-55fd-44b9-b09f-6c03b6209b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2022-05-13|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date()).show() #yyyy-MM-dd <---- Standard Date Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ff9ca2-aa93-4576-912e-d6c0bd6a88f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp()    |\n",
      "+-----------------------+\n",
      "|2022-05-13 01:12:31.539|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp()).show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS <---- Standard TimeStamp Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dced34ca-fad6-4c1b-9c4c-822c443dc262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert a string which contain date or timestamp in non-standard format to standard date or time using to_date or to_timestamp function respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d0377b-224f-489c-a8c3-0b24e26615ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   to_date|\n",
      "+----------+\n",
      "|2021-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Important: \"yyyyMMdd\" is teh format inw hich pyspark will return the date\n",
    "\n",
    "df.select(to_date(lit('20210228'), 'yyyyMMdd').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed1fe49-c475-4aa3-941d-0090ec593287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       to_timestamp|\n",
      "+-------------------+\n",
      "|2021-02-28 17:25:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('20210228 1725'), 'yyyyMMdd HHmm').alias('to_timestamp')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1dfa5-817a-463c-abd1-0dfea65fe44a",
   "metadata": {},
   "source": [
    "# Date and Time Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17503302-c4bc-4f96-8972-5a87b46fbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, add_months, round,date_add, date_sub, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a627f461-d822-498a-be8a-24923104f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from this list of tuples\n",
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad471cd0-f551-40a1-bb61-1dc7ef9b8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns shouyld be of type STRING in order to perform arithmetic and manipulations\n",
    "datetimesDF = spark.createDataFrame(datetimes, schema = \"date STRING, timestamp STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f77cc5c-096b-4547-9eb5-0157e4e982b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date|           timestamp|\n",
      "+----------+--------------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|\n",
      "|2016-02-29|2016-02-29 08:08:...|\n",
      "|2017-10-31|2017-12-31 11:59:...|\n",
      "|2019-11-30|2019-08-31 00:00:...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing everything is in order\n",
    "datetimesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9033d191-fffa-40f2-94f7-2d8f10c50938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-----------+----------+-----------+\n",
      "|      date|           timestamp|date_add_10|time_add_10|day_sub_10|time_sub_10|\n",
      "+----------+--------------------+-----------+-----------+----------+-----------+\n",
      "|2014-02-28|2014-02-28 10:00:...| 2014-03-10| 2014-03-10|2014-02-18| 2014-02-18|\n",
      "|2016-02-29|2016-02-29 08:08:...| 2016-03-10| 2016-03-10|2016-02-19| 2016-02-19|\n",
      "|2017-10-31|2017-12-31 11:59:...| 2017-11-10| 2018-01-10|2017-10-21| 2017-12-21|\n",
      "|2019-11-30|2019-08-31 00:00:...| 2019-12-10| 2019-09-10|2019-11-20| 2019-08-21|\n",
      "+----------+--------------------+-----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add 10 days to both date and time values.\n",
    "#Subtract 10 days from both date and time values.\n",
    "\n",
    "datetimesDF.select(\"date\",\"timestamp\"). \\\n",
    "withColumn(\"date_add_10\", date_add(\"date\", 10)). \\\n",
    "withColumn(\"time_add_10\", date_add(\"timestamp\", 10)). \\\n",
    "withColumn(\"day_sub_10\", date_sub(\"date\", 10)). \\\n",
    "withColumn(\"time_sub_10\", date_sub(\"timestamp\", 10)). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38990892-a7a1-483f-a6cd-8762fbf5da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function datediff in module pyspark.sql.functions:\n",
      "\n",
      "datediff(end, start)\n",
      "    Returns the number of days from `start` to `end`.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "    [Row(diff=32)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17790dd4-7d83-4b39-9451-3387fdb148dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------+---------+---------+\n",
      "|date      |timestamp              |currentDate|date_diff|time_diff|\n",
      "+----------+-----------------------+-----------+---------+---------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2022-05-13 |2996     |2996     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2022-05-13 |2265     |2265     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2022-05-13 |1655     |1594     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2022-05-13 |895      |986      |\n",
      "+----------+-----------------------+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the difference between current_date and date values as well as current_timestamp and time values.\n",
    "\n",
    "datetimesDF.select(\"date\", \"timestamp\"). \\\n",
    "withColumn(\"currentDate\", current_date()). \\\n",
    "withColumn(\"date_diff\", datediff(current_date(), \"date\")). \\\n",
    "withColumn(\"time_diff\", datediff(current_date(), \"timestamp\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d8369b-7655-4078-b27f-58a0916817ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function current_date in module pyspark.sql.functions:\n",
      "\n",
      "current_date()\n",
      "    Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "    All calls of current_date within the same query return the same value.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "278e02e2-6ad3-4f80-99eb-f74665312da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------+------------+------------+------------+------------+\n",
      "|date      |timestamp              |currentDate|date_between|time_between|date_3months|time_3months|\n",
      "+----------+-----------------------+-----------+------------+------------+------------+------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2022-05-13 |98.51612903 |98.50510827 |2014-05-28  |2014-05-28  |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2022-05-13 |74.48387097 |74.47535618 |2016-05-29  |2016-05-29  |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2022-05-13 |54.41935484 |52.40564628 |2018-01-31  |2018-03-31  |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2022-05-13 |29.4516129  |32.42177494 |2020-02-29  |2019-11-30  |\n",
      "+----------+-----------------------+-----------+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the number of months between current_date and date values as well as current_timestamp and time values.\n",
    "#Add 3 months to both date values as well as time values.\n",
    "\n",
    "datetimesDF.select(\"date\", \"timestamp\"). \\\n",
    "withColumn(\"currentDate\", current_date()). \\\n",
    "withColumn(\"date_between\", months_between(current_date(), \"date\")). \\\n",
    "withColumn(\"time_between\", months_between(current_timestamp(), \"timestamp\")). \\\n",
    "withColumn(\"date_3months\", add_months(\"date\", 3)). \\\n",
    "withColumn(\"time_3months\", add_months(\"timestamp\", 3)). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fe7fbbb-9db3-48dd-907f-0a729c1396f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function add_months in module pyspark.sql.functions:\n",
      "\n",
      "add_months(start, months)\n",
      "    Returns the date that is `months` months after `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "    [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(add_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9689a49-603f-46e9-9199-ec5e536b2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function months_between in module pyspark.sql.functions:\n",
      "\n",
      "months_between(date1, date2, roundOff=True)\n",
      "    Returns number of months between dates date1 and date2.\n",
      "    If date1 is later than date2, then the result is positive.\n",
      "    If date1 and date2 are on the same day of month, or both are the last day of month,\n",
      "    returns an integer (time of day will be ignored).\n",
      "    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "    [Row(months=3.94959677)]\n",
      "    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "    [Row(months=3.9495967741935485)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(months_between)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bd076-a733-4695-9941-ed75ba84f5d5",
   "metadata": {},
   "source": [
    "# Date and Time Trunc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f68d7ed-40a8-4ae2-afe4-49df903d1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a96638d-0460-4cbe-b691-f4b4124dd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema = \"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45a9735-ce83-4c7d-8481-656e4c369c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8d8389-b6ed-4d6a-8386-e8481342df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_trunc|time_trunc|\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get beginning month date using date field and beginning year date using time field.\n",
    "\n",
    "from pyspark.sql.functions import trunc\n",
    "\n",
    "datetimesDF. \\\n",
    "withColumn(\"date_trunc\", trunc(\"date\", \"MM\")). \\\n",
    "withColumn(\"time_trunc\", trunc(\"time\", \"yy\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f5f24-597f-4228-8a52-5df331cc5931",
   "metadata": {},
   "source": [
    "#### Difference between above and below example is **trunc()** truncates only to the date, **date_trunc()** truncates time as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e75126b1-bc20-4f76-ab16-1f77d58b8f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_trunc         |time_trunc         |\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01 00:00:00|2014-01-01 00:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01 00:00:00|2016-01-01 00:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01 00:00:00|2017-01-01 00:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01 00:00:00|2019-01-01 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get beginning hour time using date and time field\n",
    "\n",
    "from pyspark.sql.functions import date_trunc\n",
    "\n",
    "datetimesDF. \\\n",
    "withColumn(\"date_trunc\", date_trunc('MM', \"date\")). \\\n",
    "withColumn(\"time_trunc\", date_trunc('yy', \"time\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a35394-e85b-434d-ae9d-49bcd4409299",
   "metadata": {},
   "source": [
    "# Date and Time Extract Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3cd24-2b66-4cee-8e43-abfa6e42c5f4",
   "metadata": {},
   "source": [
    "Here are the common extract functions: year, month, weekofyear, dayofyear, dayofmonth, dayofweek, hour, minute, second. They are self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c863c4-108d-4d51-b38b-357ceadc60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, weekofyear, dayofmonth, \\\n",
    "    dayofyear, dayofweek, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f918193f-d3c4-4327-b5af-24824d0472f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [(\"X\", )]\n",
    "df = spark.createDataFrame(l).toDF(\"dummy\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9443b-bee0-4cb2-b9f7-f315d826cf6b",
   "metadata": {},
   "source": [
    "The following functions only work with **DATES** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a6ecb82-d6b4-4c54-a140-a260b4800034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "|current_date|year|month|weekofyear|dayofyear|dayofmonth|dayofweek|\n",
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "|2022-05-13  |2022|5    |19        |133      |13        |6        |\n",
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    current_date().alias('current_date'), \n",
    "    year(current_date()).alias('year'),\n",
    "    month(current_date()).alias('month'),\n",
    "    weekofyear(current_date()).alias('weekofyear'),\n",
    "    dayofyear(current_date()).alias('dayofyear'),\n",
    "    dayofmonth(current_date()).alias('dayofmonth'),\n",
    "    dayofweek(current_date()).alias('dayofweek')\n",
    ").show(truncate = False) #yyyy-MM-dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb28e3f-06c1-46ea-b6eb-dd102c596339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, hour, minute, second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b40bec-4dd9-431d-982e-3ba410efc8c2",
   "metadata": {},
   "source": [
    "The following functions work with **DATES** and **TIMESTAMPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253aabbe-2bba-45b4-9225-c09128e3e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|2022-05-13 15:25:41.254|2022|5    |13        |15  |25    |41    |\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    current_timestamp().alias('current_timestamp'), \n",
    "    year(current_timestamp()).alias('year'),\n",
    "    month(current_timestamp()).alias('month'),\n",
    "    dayofmonth(current_timestamp()).alias('dayofmonth'),\n",
    "    hour(current_timestamp()).alias('hour'),\n",
    "    minute(current_timestamp()).alias('minute'),\n",
    "    second(current_timestamp()).alias('second')\n",
    ").show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f72b1-f9cd-4b0e-96da-8b5c0e8bfcae",
   "metadata": {},
   "source": [
    "# Using to_date and to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449f923f-feac-451e-915b-bfaaf42dcbcd",
   "metadata": {},
   "source": [
    "We can use *to_date()* and *to_timestamp()* to convert non standard dates and timestamps to standard formats.\n",
    "1. **yyyyMMdd** is the standard date format\n",
    "2. **dd-MMM-yyyy HH:mm:ss.SSS** is the standard timestamp format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ae30d1-375c-453e-a1ca-276c91bd4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(20140228, \"28-Feb-2014 10:00:00.123\"),\n",
    "                     (20160229, \"20-Feb-2016 08:08:08.999\"),\n",
    "                     (20171031, \"31-Dec-2017 11:59:59.123\"),\n",
    "                     (20191130, \"31-Aug-2019 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a254a87-a072-4c6b-a3dd-a4cf4f7ac0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema = \"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2288fce7-b556-4046-9ef4-6904b211943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|date    |time                    |\n",
      "+--------+------------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638c1f7e-e48e-4023-ab32-db06b6c65817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_date, to_timestamp, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8d5211-d449-4528-9aa8-b8295a7aa4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n",
      "\n",
      "to_date(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c38d82c-59b9-49a6-8f9e-d9a57a6b5825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_timestamp in module pyspark.sql.functions:\n",
      "\n",
      "to_timestamp(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34b46965-d6c4-4cd3-b1e6-367d24f6ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|    date|                time|\n",
      "+--------+--------------------+\n",
      "|20140228|28-Feb-2014 10:00...|\n",
      "|20160229|20-Feb-2016 08:08...|\n",
      "|20171031|31-Dec-2017 11:59...|\n",
      "|20191130|31-Aug-2019 00:00...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data in datetimesDF to standard dates or timestamps\n",
    "\n",
    "datetimesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b726301-9740-4dd5-bdee-4faa6082b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------+-----------------------+\n",
      "|date    |time                    |to_date   |to_timestamp           |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|2016-02-29|2016-02-20 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|2019-11-30|2019-08-31 00:00:00    |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"to_date\", to_date(col(\"date\"), \"yyyyMMdd\")). \\\n",
    "    withColumn(\"to_timestamp\", to_timestamp(col(\"time\"), \"dd-MMM-yyyy HH:mm:ss.SSS\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045b0f7-2603-4ba9-8018-6886c267064a",
   "metadata": {},
   "source": [
    "# Using date_format() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc23ad8-3a02-41fb-be6a-d0e0dc71af1e",
   "metadata": {},
   "source": [
    "We can use *date_format* to extract the required information in a desired format from standard date or timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a31cb29b-7d49-4afc-8491-fc98fe409055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fb8b301-2150-4873-a69d-f8f3d3c316ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the year and month from both date and time columns using yyyyMM format.\n",
    "# Also make sure that the data type is converted to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96461d96-ddc0-48ed-ba9f-0ba4c14105f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc3ea53d-3e15-4f91-8aca-c83aedb16c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n",
      "\n",
      "date_format(date, format)\n",
      "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "    format given by the second argument.\n",
      "    \n",
      "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "    pattern letters of `datetime pattern`_. can be used.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Whenever possible, use specialized functions like `year`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "    [Row(date='04/08/2015')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1603f9f2-1f98-4116-9aaf-42307a14ac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_ym|time_ym|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_ym\", date_format(\"date\", \"yyyyMM\").cast(\"int\")). \\\n",
    "    withColumn(\"time_ym\", date_format(\"time\", \"yyyyMM\").cast(\"int\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff6f9d40-ad70-48fb-82c0-4c53ac8a30cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+--------------+\n",
      "|date      |time                   |date_ym|time_ym       |\n",
      "+----------+-----------------------+-------+--------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |20140228100000|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |20160229080808|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |20171231115959|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |20190831000000|\n",
      "+----------+-----------------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the information from time in yyyyMMddHHmmss format.\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_ym\", date_format(\"date\", \"yyyyMM\").cast(\"int\")). \\\n",
    "    withColumn(\"time_ym\", date_format(\"time\", \"yyyyMMddHHmmss\").cast(\"long\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a885bf6-c20c-4774-891a-d2b44e7be3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_yd|time_yd|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014059|2014059|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016060|2016060|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017304|2017365|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019334|2019243|\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get year and day of year uisng yyyyDDD format\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_yd\", date_format(\"date\", \"yyyyDDD\").cast(\"int\")). \\\n",
    "    withColumn(\"time_yd\", date_format(\"time\", \"yyyyDDD\").cast(\"int\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad3799ba-09ad-4f14-8352-120991d25437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------+\n",
      "|date      |time                   |date_description |\n",
      "+----------+-----------------------+-----------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|February 28, 2014|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|February 29, 2016|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|October 31, 2017 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|November 30, 2019|\n",
      "+----------+-----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get complete description of the date\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_description\", date_format(\"date\", \"MMMM dd, yyyy\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59607691-4ec1-4720-b60c-d836305899de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+---------+\n",
      "|date      |time                   |date_abbr|date_full|\n",
      "+----------+-----------------------+---------+---------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|Fri      |Friday   |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|Mon      |Monday   |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|Tue      |Tuesday  |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|Sat      |Saturday |\n",
      "+----------+-----------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get name of the week day using date.\n",
    "# EE = Abbreviated name, EEEE = Full name\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_abbr\", date_format(\"date\", \"EE\")). \\\n",
    "    withColumn(\"date_full\", date_format(\"date\", \"EEEE\")). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42200c2-0265-42b5-a248-c599b2081243",
   "metadata": {},
   "source": [
    "# Dealing with Unix Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d3651-22fb-4d48-8a69-614720813cfc",
   "metadata": {},
   "source": [
    "Unix Timestamp is an integer that started from January 1st 1970 Midnight UTC, also known as epoch, and is incremented by 1 every second.\n",
    "\n",
    "We can convert regular date/timestamp to Unix Timestamp using **unix_timestamp**.\n",
    "\n",
    "We can convert to regular date/timestamp from Unix using **from_unixtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95974521-107f-4f7f-ba2e-def611ecef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+\n",
      "|dateid  |date      |time                   |\n",
      "+--------+----------+-----------------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+--------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [(20140228, \"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (20160229, \"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (20171031, \"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (20191130, \"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes).toDF(\"dateid\", \"date\", \"time\")\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f567aea3-c5db-4a6a-a23e-5f5fc2e7ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0eaea1-ea16-470c-b4e4-5084b52b9694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
